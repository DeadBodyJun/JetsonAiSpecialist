# JetsonAiSpecialist
# Nvidia AI Specialist Certification

<aside>
ğŸ’¡

**OverView of the Project**

í”„ë¡œì íŠ¸ ê°œìš”

- Opening background information (ë‚´ í”„ë¡œì íŠ¸ì˜ ì „ë°˜ì ì¸ ë¬¸ë§¥ì„ ìœ„í•´ì„œ í•„ìš”!)

- General description of the current project (í”„ ë¡œì íŠ¸ì˜ ì „ë°˜ì ì¸ ì„¤ëª…)

- Proposed idea for enhancements to the project (ì œì•ˆí•˜ê³  ì‹¶ì€ í”„ë¡œì íŠ¸ì˜ ê°•ì )

- Value and significance of this project (ì¤‘ìš”ì„±)

- Current limitations (ì§ë©´í•˜ê³  ìˆëŠ” í•œê³„)

- Literature review(ì „ë°˜ì ì¸ í”„ë¡œì íŠ¸ì˜ ë°°ê²½ì§€ì‹ ê³µìœ ë¥¼ ìœ„í•´ì„œ!)

</aside>

# Title

[ì£¼ì œ]

DoorSense: íœ´ë¨¸ë…¸ì´ë“œ ë¡œë´‡ì˜ ë¬¸ ì¸ì‹ ì‹œìŠ¤í…œ

# Opening background information

[ ë°°ê²½ ì •ë³´ ]

<aside>
âœ… íœ´ë¨¸ë…¸ì´ë“œ ë¡œë´‡ì´ ì‹¤ë‚´ í™˜ê²½ì—ì„œ ììœ¨ì ìœ¼ë¡œ ì´ë™í•˜ê³  ì‘ì—…í•˜ê¸° ìœ„í•´ ì£¼ë³€ì˜ ì¥ì• ë¬¼ê³¼ ì‚¬ë¬¼ì„ ì¸ì‹í•˜ê³  í”¼í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì´ í•„ìš”í•˜ì˜€ìŒ. ì´ë¥¼ ìœ„í•´ ë¬¸ê³ ë¦¬ì™€ ê°™ì€ íŠ¹ì • ì‚¬ë¬¼ì„ ì¸ì‹í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ì˜€ê³ , ë¬¸ê³¼ ê°™ì€ êµ¬ì¡°ë¬¼ì„ ì „ì²´ì ìœ¼ë¡œ ì¸ì§€í•¨ìœ¼ë¡œì¨ ë¡œë´‡ì´ ì´ë™ ê²½ë¡œë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì„¤ì •í•˜ê³  í•„ìš”ì— ë”°ë¼ íšŒí”¼í•  ìˆ˜ ìˆë„ë¡ í•˜ì˜€ìŒ. YOLOv5ì™€ ê°™ì€ ë”¥ëŸ¬ë‹ ê¸°ë°˜ ê°ì²´ ì¸ì‹ ê¸°ìˆ ì€ ì´ëŸ¬í•œ ê¸°ëŠ¥ì„ êµ¬í˜„í•˜ëŠ” ë° ì í•©í•˜ì˜€ê³ , íŠ¹íˆ ì‹¤ì‹œê°„ìœ¼ë¡œ ë¬¸ê³ ë¦¬ì™€ ê°™ì€ ì‘ì€ ë¬¼ì²´ë„ ì •í™•íˆ íƒì§€í•  ìˆ˜ ìˆì–´ ë¡œë´‡ì´ í™˜ê²½ì„ ì´í•´í•˜ê³  ì ì‘í•˜ë„ë¡ í•˜ì˜€ìŒ. ë³¸ í”„ë¡œì íŠ¸ëŠ” YOLOv5ë¥¼ í™œìš©í•´ ë¬¸ê³ ë¦¬ë¥¼ ì¸ì‹í•˜ê³ , ì´ë¥¼ íœ´ë¨¸ë…¸ì´ë“œ ë¡œë´‡ì˜ ì¥ì• ë¬¼ ì¸ì‹ ì‹œìŠ¤í…œì— ì ìš©í•¨ìœ¼ë¡œì¨ ìŠ¤ë§ˆíŠ¸í™ˆ, ì„œë¹„ìŠ¤ ë¡œë´‡ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ë¡œë´‡ í™œìš© ê°€ëŠ¥ì„±ì„ ë†’ì´ê³ ì í•¨.

</aside>

- Humanoid robots require the ability to recognize and avoid obstacles and objects in their surroundings to autonomously navigate and operate within indoor environments. For this purpose, recognizing specific objects, such as door handles, is essential, allowing the robot to perceive entire door structures and efficiently plan its routes while avoiding obstacles when necessary. YOLOv5, a deep learning-based object recognition technology, is well-suited to implement these capabilities, enabling the robot to accurately detect small objects like door handles in real-time, thereby enhancing its understanding and adaptation to its environment. This project aims to apply YOLOv5 to recognize door handles and integrate this capability into the humanoid robot's obstacle detection system, thereby increasing the potential for robot utilization in various fields such as smart homes and service robots.
- 

# General description of the current project

[í”„ë¡œì íŠ¸ ì „ë°˜ì ì¸ ì„¤ëª…]

<aside>
âœ… ì´ í”„ë¡œì íŠ¸ëŠ” íœ´ë¨¸ë…¸ì´ë“œ ë¡œë´‡ì´ ì‹¤ë‚´ í™˜ê²½ì—ì„œ ììœ¨ì ìœ¼ë¡œ ë¬¸ì„ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ì‹œìŠ¤í…œì„ ê°œë°œí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤. ë¬¸ê³ ë¦¬ë¥¼ ë‹¤ì–‘í•œ ê°ë„ì™€ ì¡°ê±´ì—ì„œ ì´¬ì˜í•˜ì—¬ YOLOv5 ê¸°ë°˜ ëª¨ë¸ì— í•™ìŠµì‹œí‚¤ê³ , ì´ í•™ìŠµ ëª¨ë¸ì„ í†µí•´ ë¡œë´‡ ì „ë°©ì˜ íŠ¹ì • ê±°ë¦¬(`n` ê°’ìœ¼ë¡œ ì„¤ì •ëœ ë²”ìœ„) ë‚´ì—ì„œ ë¬¸ê³ ë¦¬ë¥¼ ì¸ì‹í•˜ê²Œ í•œë‹¤. ë¬¸ê³ ë¦¬ê°€ ê°ì§€ë˜ë©´ ë¡œë´‡ì€ ì´ë¥¼ "ë¬¸ì´ ìˆë‹¤"ë¼ê³  íŒë‹¨í•˜ì—¬, ê²½ë¡œë¥¼ íšŒí”¼í•˜ê±°ë‚˜ ë¬¸ì„ ì—¬ëŠ” ë™ì‘ì„ ì¤€ë¹„í•˜ëŠ” ë“± ìƒí™©ì— ë§ëŠ” ë°˜ì‘ì„ í•  ìˆ˜ ìˆë‹¤. ì´ í”„ë¡œì íŠ¸ëŠ” íœ´ë¨¸ë…¸ì´ë“œ ë¡œë´‡ì˜ ë‚´ë¹„ê²Œì´ì…˜ ëŠ¥ë ¥ì„ í–¥ìƒì‹œì¼œ, ìŠ¤ë§ˆíŠ¸í™ˆê³¼ ì„œë¹„ìŠ¤ ë¡œë´‡ ë¶„ì•¼ ë“±ì—ì„œ ììœ¨ ì´ë™ì„±ì„ ë†’ì´ê³  ë‹¤ì–‘í•œ í™œìš© ê°€ëŠ¥ì„±ì„ ì—´ì–´ì¤„ ìˆ˜ ìˆë‹¤.

</aside>

- This project aims to develop a system that enables humanoid robots to autonomously recognize doors in indoor environments. By capturing images of door handles from various angles and conditions and training a YOLOv5-based model, the robot can recognize door handles within a specific distance (defined as an `n` value) in front of it. When a door handle is detected, the robot interprets this as a "door present," allowing it to respond appropriately by either avoiding the path or preparing to open the door. This project enhances the navigation capabilities of humanoid robots, expanding their potential applications in fields like smart homes and service robotics, ultimately improving autonomous mobility.

### Value and signifiance of the project

[ í”„ë¡œì íŠ¸ì˜ ì¤‘ìš”ì„±] 

<aside>
ğŸ’¡

ì´ í”„ë¡œì íŠ¸ëŠ” ì‹¤ë‚´ ììœ¨ ë¡œë´‡ì˜ ë°œì „ê³¼ ì‚¬ìš©ì ì ‘ê·¼ì„± í–¥ìƒì— ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. ë¬¸ê³ ë¦¬ì™€ ê°™ì€ ì‘ì€ ë¬¼ì²´ë¥¼ ì¸ì‹í•¨ìœ¼ë¡œì¨ ë¡œë´‡ì´ ë¬¸ê³¼ ê°™ì€ êµ¬ì¡°ë¬¼ì„ íƒì§€í•˜ê³  íšŒí”¼í•˜ê±°ë‚˜ ìƒí˜¸ì‘ìš©í•  ìˆ˜ ìˆì–´, ìŠ¤ë§ˆíŠ¸í™ˆ, ë³‘ì›, í˜¸í…” ë“± ë‹¤ì–‘í•œ ì‹¤ë‚´ í™˜ê²½ì—ì„œ ìœ ì—°í•˜ê²Œ ì‘ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ê³ ë„í™”ëœ ì¥ì• ë¬¼ ì¸ì‹ ê¸°ìˆ ì„ í†µí•´ ë³µì¡í•œ í™˜ê²½ì—ì„œë„ ì•ˆì •ì ìœ¼ë¡œ ì´ë™í•˜ë©°, ì´ë¥¼ í†µí•´ ê³ ë ¹ìì™€ ì¥ì• ì¸ì—ê²Œë„ ë” ë†’ì€ ì ‘ê·¼ì„±ì„ ì œê³µí•˜ì—¬ ì‹¤ìš©ì ì´ê³  ë§ì¶¤í˜• ì„œë¹„ìŠ¤ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ë˜í•œ, ì´ í”„ë¡œì íŠ¸ëŠ” ììœ¨ ë¡œë´‡ì´ ë‹¤ì–‘í•œ ì‹¤ë‚´ì™¸ í™˜ê²½ì—ì„œ ì‚¬ëŒì„ ë•ê³  ì„ë¬´ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ê¸°ì´ˆ ê¸°ìˆ ë¡œ ìë¦¬ ì¡ì•„, í–¥í›„ ë¡œë´‡ì˜ í™œìš©ì„±ì„ ë”ìš± í™•ëŒ€í•  ì ì¬ë ¥ì´ ìˆìŠµë‹ˆë‹¤.

</aside>

- This project plays a key role in advancing indoor autonomous robots and enhancing user accessibility. By recognizing small objects like door handles, the robot can detect, avoid, or interact with structures like doors, allowing it to operate flexibly in various indoor environments such as smart homes, hospitals, and hotels. Particularly, this advanced obstacle detection technology enables stable navigation even in complex settings, providing greater accessibility for elderly and disabled users, allowing for practical and personalized services. Furthermore, this project establishes foundational technology for autonomous robots to assist people and perform tasks in diverse environments, offering significant potential for expanded robot applications in the future.

### **Current limitations**

[ì§ë©´í•˜ê³  ìˆëŠ” í•œê³„]

<aside>
âœ… ì´ í”„ë¡œì íŠ¸ëŠ” ë‹¤ì–‘í•œ í•œê³„ì— ì§ë©´í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë¨¼ì €, ë¬¸ê³ ë¦¬ëŠ” ëª¨ì–‘, ìƒ‰ìƒ, ìœ„ì¹˜, ì¡°ëª… ì¡°ê±´ì— ë”°ë¼ ì¸ì‹ì´ ì–´ë ¤ì›Œì§ˆ ìˆ˜ ìˆì–´ ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ì¼ê´€ëœ ì„±ëŠ¥ì„ ìœ ì§€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë˜í•œ, í˜„ì¬ ì„¤ì •ëœ ê±°ë¦¬ ë‚´ì—ì„œë§Œ ë¬¸ê³ ë¦¬ë¥¼ íƒì§€í•  ìˆ˜ ìˆì–´ ë¡œë´‡ì´ ë©€ë¦¬ ìˆëŠ” ë¬¸ê³ ë¦¬ë¥¼ ì¸ì‹í•˜ëŠ” ë° í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. ì‹¤ì‹œê°„ ê°ì²´ ì¸ì‹ì—ëŠ” ë†’ì€ ê³„ì‚° ìì›ì´ ìš”êµ¬ë˜ë©°, ì´ëŠ” íœ´ë¨¸ë…¸ì´ë“œ ë¡œë´‡ì˜ ë°°í„°ë¦¬ ì†Œëª¨ì™€ ë¹„ìš© ì¦ê°€ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”ë¶ˆì–´, ë¬¸ê³ ë¦¬ë¥¼ ì¸ì‹í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œëŠ” ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©° ì‹¤ì œë¡œ ë¬¸ì„ ì—¬ëŠ” ë™ì‘ì„ ìˆ˜í–‰í•˜ë ¤ë©´ ì¶”ê°€ì ì¸ ê¸°ê³„ì  ì„¤ê³„ê°€ í•„ìš”í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ í˜•íƒœì˜ ë¬¸ê³ ë¦¬ë¥¼ ì¸ì‹í•˜ê¸° ìœ„í•´ì„œëŠ” ë°©ëŒ€í•œ ë°ì´í„°ì™€ í•™ìŠµ ì‹œê°„ì´ ìš”êµ¬ë˜ì–´ í”„ë¡œì íŠ¸ ì§„í–‰ì— ì–´ë ¤ì›€ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

</aside>

- This project faces several limitations. First, recognizing door handles can be challenging due to variations in shape, color, position, and lighting conditions, making it difficult to maintain consistent performance across different environments. Additionally, the system can only detect door handles within a set range, limiting the robot's ability to recognize handles at a distance. Real-time object detection requires high computational resources, which can lead to increased battery consumption and costs for humanoid robots. Furthermore, simply recognizing the door handle is insufficient; additional mechanical design is required for the robot to actually open doors. To recognize various types of door handles, extensive data and training time are required, posing challenges for project progress.

### **Literature review**

[ë¬¸í—Œ ê³ ì°°]

<aside>
ğŸ’¡

1. **ë¡œë´‡ì˜ ì‹¤ë‚´ ë‚´ë¹„ê²Œì´ì…˜**
    
    ì‹¤ë‚´ í™˜ê²½ì—ì„œ ììœ¨ì ìœ¼ë¡œ ì´ë™í•˜ëŠ” ë¡œë´‡ì˜ ë‚´ë¹„ê²Œì´ì…˜ ê¸°ìˆ ì€ ì§€ì†ì ì¸ ì—°êµ¬ì˜ ëŒ€ìƒì´ ë˜ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ë¡œë´‡ì´ ì‚¬ëŒì˜ ë„ì›€ ì—†ì´ë„ ì£¼ë³€ í™˜ê²½ì„ ì´í•´í•˜ê³  ì¥ì• ë¬¼ì„ íšŒí”¼í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì€ ìŠ¤ë§ˆíŠ¸í™ˆ, ì˜ë£Œì‹œì„¤, ìƒì—… ê³µê°„ ë“±ì—ì„œ ë¡œë´‡ì˜ í™œìš© ê°€ëŠ¥ì„±ì„ í™•ëŒ€í•˜ê³  ìˆìŠµë‹ˆë‹¤. ìµœê·¼ ì—°êµ¬ë“¤ì€ ë¡œë´‡ì˜ ììœ¨ ë‚´ë¹„ê²Œì´ì…˜ì„ ìœ„í•´ SLAM(Simultaneous Localization and Mapping), ë ˆì´ì € ì„¼ì„œ, ì´ˆìŒíŒŒ ì„¼ì„œ, ì¹´ë©”ë¼ ë¹„ì „ì„ ê²°í•©í•œ ë‹¤ì–‘í•œ ì ‘ê·¼ ë°©ì‹ì„ í™œìš©í•˜ê³  ìˆìœ¼ë©°, íŠ¹íˆ ì¹´ë©”ë¼ ë¹„ì „ ê¸°ë°˜ì˜ ë‚´ë¹„ê²Œì´ì…˜ ì‹œìŠ¤í…œì´ ì‹¤ë‚´ ë¡œë´‡ ë‚´ë¹„ê²Œì´ì…˜ì—ì„œ ì¤‘ìš”í•œ ìš”ì†Œë¡œ ìë¦¬ ì¡ê³  ìˆìŠµë‹ˆë‹¤.
    
2. **ê°ì²´ ì¸ì‹ ê¸°ìˆ **
    
    ê°ì²´ ì¸ì‹ ê¸°ìˆ ì€ ë¡œë´‡ì´ íŠ¹ì • ë¬¼ì²´ë¥¼ ì‹ë³„í•˜ê³  ì¸ì‹í•˜ëŠ” ë° í•µì‹¬ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤. íŠ¹íˆ YOLO(You Only Look Once)ì™€ ê°™ì€ ì‹¤ì‹œê°„ ê°ì²´ íƒì§€ ëª¨ë¸ì€ ì‹ ì†í•˜ê³  ì •í™•í•œ ê°ì²´ ì¸ì‹ì„ ìœ„í•´ ë§ì´ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤. YOLOv5ëŠ” ì´ì „ ë²„ì „ë³´ë‹¤ ì„±ëŠ¥ê³¼ ì†ë„ê°€ í–¥ìƒëœ ëª¨ë¸ë¡œ, ë‹¤ì–‘í•œ ê°ì²´ ì¸ì‹ ì—°êµ¬ì—ì„œ ì‹¤ì‹œê°„ ì¸ì‹ì´ í•„ìš”í•œ ì‘ìš© ë¶„ì•¼ì— ì í•©í•œ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤. ì—°êµ¬ì— ë”°ë¥´ë©´, YOLOv5ëŠ” ì‘ì€ ë¬¼ì²´ì˜ ì¸ì‹ ì •í™•ë„ê°€ ë†’ì•„ ë¡œë´‡ì´ ì‹¤ë‚´ì—ì„œ ë¬¸ê³ ë¦¬ì™€ ê°™ì€ íŠ¹ì • ë¬¼ì²´ë¥¼ ì‹ë³„í•˜ëŠ” ë° íš¨ê³¼ì ì…ë‹ˆë‹¤.
    
3. **ì¥ì• ë¬¼ íƒì§€ì™€ ììœ¨ ì´ë™**
    
    ììœ¨ ì´ë™ ë¡œë´‡ì˜ ê°œë°œì—ì„œ ì¥ì• ë¬¼ íƒì§€ëŠ” ë§¤ìš° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ë§ì€ ì—°êµ¬ë“¤ì´ ì„¼ì„œ ê¸°ë°˜ íƒì§€ ê¸°ìˆ ê³¼ ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì¸ì‹ ê¸°ìˆ ì„ ê²°í•©í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ì„¼ì„œ íƒì§€ì™€ëŠ” ë‹¬ë¦¬, ë”¥ëŸ¬ë‹ ê¸°ë°˜ íƒì§€ ê¸°ìˆ ì€ ë¡œë´‡ì´ ì£¼ë³€ í™˜ê²½ì„ ë” ì •êµí•˜ê²Œ ì¸ì‹í•˜ê³ , ì˜ˆì¸¡ ê°€ëŠ¥í•œ ê²½ë¡œë¥¼ ê³„íší•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤. ë¬¸ê³ ë¦¬ì™€ ê°™ì€ ì‘ì€ ë¬¼ì²´ë¥¼ í¬í•¨í•œ ì¥ì• ë¬¼ íƒì§€ ì‹œìŠ¤í…œì€ íŠ¹íˆ ì‹¤ë‚´ í™˜ê²½ì—ì„œ ì¥ì• ë¬¼ ì¸ì‹ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³ , ë¡œë´‡ì˜ ì´ë™ì„±ì„ ë†’ì´ê¸° ìœ„í•œ ì¤‘ìš”í•œ ì—°êµ¬ ê³¼ì œë¡œ ë‹¤ë¤„ì§€ê³  ìˆìŠµë‹ˆë‹¤.
    
4. **ë¯¸ë˜ì˜ ì‘ìš© ê°€ëŠ¥ì„±**
    
    ì´ëŸ¬í•œ ê°ì²´ ì¸ì‹ ë° ë‚´ë¹„ê²Œì´ì…˜ ê¸°ìˆ ì˜ ë°œì „ì€ íœ´ë¨¸ë…¸ì´ë“œ ë¡œë´‡ì˜ í™œìš©ì„ ìœ„í•œ ì¤‘ìš”í•œ ê¸°ë°˜ì´ ë©ë‹ˆë‹¤. í˜„ì¬ ìŠ¤ë§ˆíŠ¸í™ˆ, ì˜ë£Œ ì‹œì„¤, ì„œë¹„ìŠ¤ ì‚°ì—…ì—ì„œ ë¡œë´‡ í™œìš©ì˜ í•„ìš”ì„±ì´ ì¦ê°€í•˜ê³  ìˆìœ¼ë©°, ììœ¨ ì´ë™ ëŠ¥ë ¥ì„ ê°€ì§„ ë¡œë´‡ì€ ì´ëŸ¬í•œ ìš”êµ¬ë¥¼ ì¶©ì¡±í•˜ëŠ” ì¤‘ìš”í•œ ì—­í• ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¬¸ê³ ë¦¬ ì¸ì‹ í”„ë¡œì íŠ¸ëŠ” ì´ëŸ¬í•œ ì—°êµ¬ íë¦„ì˜ ì¼í™˜ìœ¼ë¡œ, ì‹¤ë‚´ í™˜ê²½ì—ì„œ ë¡œë´‡ì´ ë…ë¦½ì ìœ¼ë¡œ ë¬¼ì²´ë¥¼ ì¸ì‹í•˜ê³  ìƒí˜¸ì‘ìš©í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ë†’ì´ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•˜ì—¬ ììœ¨ ë¡œë´‡ì˜ ì‹¤ìš©ì„±ì„ í™•ì¥í•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤.
    
</aside>

1. **Indoor Robot Navigation**
    
    Navigation technology for robots in indoor environments is a constant focus of research, especially as the ability of robots to understand surroundings and avoid obstacles independently increases their potential for use in smart homes, healthcare facilities, and commercial spaces. Recent studies incorporate various approaches, such as combining SLAM (Simultaneous Localization and Mapping), laser sensors, ultrasonic sensors, and camera vision for autonomous robot navigation, with camera vision-based navigation becoming a key component for indoor robot navigation.
    
2. **Object Recognition Technology**
    
    Object recognition plays a crucial role in enabling robots to identify specific objects. Models like YOLO (You Only Look Once) are widely used for fast, accurate object detection, with YOLOv5 offering enhanced performance and speed compared to previous versions. Research shows that YOLOv5 is particularly effective for real-time recognition, especially for small objects, making it suitable for identifying specific items like door handles in indoor environments.
    
3. **Obstacle Detection and Autonomous Movement**
    
    Obstacle detection is critical in developing autonomous robots. Numerous studies combine sensor-based detection and deep learning-based recognition technology, with deep learning helping robots more precisely interpret their surroundings and plan routes predictively. Obstacle detection systems capable of recognizing small objects like door handles are essential for overcoming limitations in indoor obstacle recognition and improving robot mobility.
    
4. **Future Applications**
    
    Advances in object recognition and navigation technology provide a solid foundation for the application of humanoid robots. The need for robots in smart homes, healthcare, and service industries is growing, and autonomous robots with navigation capabilities are poised to meet these demands. This project aligns with this research trend, aiming to improve robots' ability to independently recognize and interact with objects in indoor settings, suggesting greater utility for autonomous robots.
    

## **ì˜ìƒ ì·¨ë“ ë°©ë²• (Image Acquisition Method):**

- ì‹¤ì œ ì‹¤ë‚´ì˜ ë¬¸ê³ ë¦¬ë¥¼ ë‹¤ê°ë„ë¡œ ì´¬ì˜ í•˜ì˜€ë‹¤
Captured actual indoor door handles from various angles.

(https://youtube.com/shorts/IZqbfBX37iw?feature=share)

## í•™ìŠµ ë°ì´í„° ì¶”ì¶œê³¼ í•™ìŠµ ì–´ë…¸í…Œì´ì…˜ (**Learning Data Extraction and Learning Annotation)**:

YOLOv5ì—ì„œ 640 í•´ìƒë„ ì´ë¯¸ì§€ë¡œ í•™ìŠµí•˜ê¸° ìœ„í•´ì„œ ë¨¼ì € ì˜ìƒì„ 640 x 640 í•´ìƒë„ ì˜ìƒìœ¼ë¡œ ë§Œë“¤ì—ˆë‹¤.  

To learn from YOLOv5 with 640 resolution images, we first made the images into 640 x 640 resolution images.

### ë¹„ë””ì˜¤ í•´ìƒ ì¡°ì • (Video resolution adjustment)

![image.png](image.png)

640 x 640 í•´ìƒë„ë¡œ ë§Œë“¤ì–´ì§„ ì˜ìƒì„ í”„ë ˆì„ ë‹¨ìœ„ë¡œ ì´ë¯¸ì§€ë¡œ ë§Œë“¤ê±°ë‚˜ ì–´ë…¸í…Œì´ì…˜ì„ í•˜ê¸° ìœ„í•´ì„œ Video/Image Labeling and Annotation Toolë¡œ ì˜ ì•Œë ¤ì§„ DarkLabelì„ ì‚¬ìš©í–ˆë‹¤.

DarkLabel, also known as Video/Image Labeling and Annotation Tool, was used to image or annotate images made at 640 x 640 resolution in frame units.

![image.png](image%201.png)

DarkLabel í”„ë¡œê·¸ë¨ì—ì„œ ì˜ìƒì„ í”„ë ˆì„ ë‹¨ìœ„ë¡œ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤. ë¨¼ì € Open Videoë¥¼ í†µí•´ 640 x 640 í•´ìƒë„ ì˜ìƒì„ ì„ íƒí•œë‹¤. ì´í›„ labeled frames onlyê°€ ì²´í¬ í‘œì‹œê°€ í™œì„±í™” ë˜ì–´ ìˆì„í…ë° ì²´í¬ í‘œì‹œë¥¼ ë¹„í™œì„±í™”í•œë‹¤. ì´í›„ as imagesë¥¼ í†µí•´ imagesë¼ëŠ” í´ë” ì•ˆì— ì´ë¯¸ì§€ë¡œ ë³€í™˜í•œë‹¤.

In the DarkLabel program, an image can be converted into an image in units of frames. First, a 640 x 640 resolution image is selected through Open Video. After that, labeled frames only will have the check mark enabled but deactivate the check mark. After that, it is converted into an image in a folder called images through as images


í•™ìŠµì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
```
import torch
import os
from IPython.display import Image, clear_output  # to display images
```

í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
```
python "C:\\Users\\ASUS\\Desktop\\AI\\yolov5\\yolov5\\train.py"  --img 512 --batch 16 --epochs 300 --data C:\Users\ASUS\Desktop\AI\yolov5\yolov5\data.yaml --weights yolov5n.pt --cache
```

<img width="858" alt="KakaoTalk_20241117_222321158" src="https://github.com/user-attachments/assets/f8bf26cc-afc7-4d91-ba4f-8d2bb23b8d6b">

cmd í™˜ê²½ì—ì„œ í•™ìŠµì„ ëŒë¦¬ëŠ” ëª¨ìŠµ



```img 512```: ì…ë ¥ ì´ë¯¸ì§€ì˜ í¬ê¸°ë¥¼ 512*512ë¡œ ì„¤ì •í•œë‹¤.
Set the size of the input image to 640x640.

```batch 16```: ë°°ì¹˜ í¬ê¸°ë¥¼ ì„¤ì •í•œë‹¤. í•œ ë²ˆì— ì²˜ë¦¬ë˜ëŠ” ì´ë¯¸ì§€ì˜ ìˆ˜ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.
Sets the batch size. This indicates the number of images to be processed at one time.

```epochs 300```: í•™ìŠµí•  ì´ ì—í­(epoch) ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
Sets the total number of epochs to learn.

```C:\Users\ASUS\Desktop\AI\yolov5\yolov5\data.yaml```: ë°ì´í„°ì…‹ ë° ëª¨ë¸ êµ¬ì„±ì— ëŒ€í•œ ì„¤ì •ì´ ë‹´ê¸´ YAML íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì§€ì •í•œë‹¤.
Specifies the path to the YAML file containing the configuration settings for the YOLOv5 model.

```weights yolov5n.pt```: ë¯¸ë¦¬ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì§€ì •í•œë‹¤. ì—¬ê¸°ì„œëŠ” yolov5n.pt íŒŒì¼ì„ ì‚¬ìš©í•˜ê³  ìˆë‹¤.
Specifies the path to the pre-trained weights file. Here, we are using the yolov5n.pt file.


Nvidia Jetson Nano í•™ìŠµ ê²°ê³¼ Nvidia Jetson Nano Training Result
í•™ìŠµ ê²°ê³¼ëŠ” ```C:\Users\ASUS\Desktop\AI\yolov5\yolov5\runs/train```ì— ì €ì¥ëœë‹¤.
Training results are stored in ```C:\Users\ASUS\Desktop\AI\yolov5\yolov5\runs/train```

![F1_curve](https://github.com/user-attachments/assets/27ee4581-89eb-434f-a4ea-1f97f3b77978)
![PR_curve](https://github.com/user-attachments/assets/278371ed-1620-4129-8320-96e38b511e37)
![P_curve](https://github.com/user-attachments/assets/3411faf9-003c-48cb-acb1-2f961d5abfea)
![R_curve](https://github.com/user-attachments/assets/ac18c2af-10b3-4355-9f66-dc6b10341832)

![results](https://github.com/user-attachments/assets/c21f2240-7f10-40c4-8152-3a18985fc0b8)


Nvidia Jetson Nano í•™ìŠµ ê²°ê³¼ ê²€ì¦ ì˜ìƒ Nvidia Jetson Nano Training Results Verification Video

ì¹´ë©”ë¼ë¥¼ ë’¤ì§‘ì–´ë„ ì •í™•í•˜ê²Œ ë¬¸ê³ ë¦¬ë¥¼ ê°ì§€í•´ë‚¸ë‹¤

JetsonAiSpecialist verification1
https://youtube.com/shorts/6gejM1pBcUc?feature=share

JetsonAiSpecialist verification2
https://youtube.com/shorts/sqgctFothtA?feature=share



